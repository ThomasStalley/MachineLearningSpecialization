{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Linear Regression Cost Function:} $$\n",
    "\n",
    "$$ \n",
    "J(\\vec{w}, b) = \\frac{1}{2m} \\Sigma_{i=1}^{m} \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)} \\right) ^2 + \\frac{ \\lambda }{2m} \\Sigma_{j=1}^{n} w^2_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Logistic Regression Cost Function:} $$\n",
    "\n",
    "$$ J(\\vec{w}, b) = \\frac{-1}{m} \\Sigma_{i=1}^{m} \\left[ y \\log{ \\left( f_{\\vec{w}, b}(\\vec{x}^{(i)}) \\right) } + (1 - y^{(i)}) \\log{ \\left( 1 - f_{\\vec{w}, b}(\\vec{x}^{(i)}) \\right)} \\right] + \\frac{ \\lambda }{2m} \\Sigma_{j=1}^{n} w^2_j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Underfitting (~high bias):**_\\\n",
    "&rarr; model does not fit the training set well.\\\n",
    "&rarr; perhaps a preconception/bias (i.e. trying fit linear plot on polynomial dataset).\n",
    "\n",
    "_**Overfitting (~high variance):**_\\\n",
    "&rarr; model fits to training data very well, cost function approaches zero.\\\n",
    "&rarr; model will perform poorly on test or real world data.\\\n",
    "&rarr; one small change in training data, large change in model - high variance.\n",
    "\n",
    "_**Addressing Overfitting:**_\\\n",
    "&rarr; best solution is to collect more training data.\\\n",
    "&rarr; look into features to include/exclude. perhaps smaller subset of most relevant features.\\\n",
    "&rarr; can use regularization, reduce impacts of some features, can shrink certain feature weights (w_i, generally leave b).\n",
    "\n",
    "_**Regularization:**_\\\n",
    "&rarr; lambda = regularization parameter.\\\n",
    "&rarr; can even add extra term below: (lambda/2m) * (b^2).\\\n",
    "&rarr; by trying to minimise the loss term below, we fit data best as we can.\\\n",
    "&rarr; by trying to minimise the regularization term below, will keep w_j small.\\\n",
    "&rarr; so lambda will balance out both goals, fit well, without overfitting.\\\n",
    "&rarr; lambda = 0 -> model will overfit.\\\n",
    "&rarr; lambda very large -> model will underfit.\n",
    "\n",
    "_**Reminder:**_\\\n",
    "! we want to find w and b to minimuse cost function J.\\\n",
    "! this is done via gradient descent (simultanious updating of w and b).\\\n",
    "! we can now include w regularization term in gradient descent process, to prevent overfitting.\\\n",
    "! same process for linear or logistic regression, just recall that they have different cost functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
